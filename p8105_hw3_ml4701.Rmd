---
title: "P8105 Homework 3"
author: Mengfan Luo (ml4701)
output: github_document
---


```{r, echo=FALSE,message=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp =.9,
  out.width = "90%"
)
```


### Problem 1

Let's load the `instacart` data and view the first 10 lines of it:

```{r}
data("instacart")
knitr::kable(head(instacart,10))
```

#### EDA

The `intacart` dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns. There's `r sum(is.na(instacart))` missing value. 

The `r ncol(instacart)` variables include 4 character variables and 11 numeric variables, which are:

**numeric variables:**

* `order_id`: despite consist of numbers, this should be an character variable given it's a series number used for identifying different orders, rather then have any quantitative information. There are `r n_distinct(pull(instacart,order_id))` distinct values of `order_id`.

Some other numeric variable here have the issues, including:

* `product_id`: `r n_distinct(pull(instacart,product_id))` distinct values.
* `add_to_cart_order`: `r n_distinct(pull(instacart,add_to_cart_order))` distinct values.
* `reordered`: `r n_distinct(pull(instacart,reordered))` distinct values.
* `user_id`: `r n_distinct(pull(instacart,user_id))` distinct values.
* `aisle_id`: `r n_distinct(pull(instacart,aisle_id))` distinct values.
* `department_id`: `r n_distinct(pull(instacart,department_id))` distinct values.

Other "true" numeric variables are:

* `order_number`: numeric variable with mean `r mean(pull(instacart,order_number))` and range [`r range(pull(instacart,order_number))`]. 
* `days_since_prior_order`: numeric variable with mean `r mean(pull(instacart, days_since_prior_order))` and range [`r range(pull(instacart, days_since_prior_order))`]. 
* `order_dow`: numeric (integer) variable with mean `r mean(pull(instacart, order_dow))` and range [`r range(pull(instacart, order_dow))`], which stands for Sunday to Saturday. 
* `order_hour_of_day`: numeric (integer) variable with mean `r mean(pull(instacart, order_hour_of_day))` and range [`r range(pull(instacart, order_hour_of_day))`], which stands for certain hours in a day. 


**character variables:**

* `eval_set`: `r n_distinct(pull(instacart,eval_set))` distinct value. This variable may not be so crucial since it only has `r n_distinct(pull(instacart,eval_set))` unique value ``r unique(pull(instacart,eval_set))``.
* `product_name`: `r n_distinct(pull(instacart,product_name))` distinct values.
* `aisle`: `r n_distinct(pull(instacart,aisle))` distinct values.
* `department`: `r n_distinct(pull(instacart,department))` distinct values.

Some variables have a one-to-one mapping relationship, including

* `product_id` and `product_name`
* `aisle_id` and `aisle`
* `department_id` and `department`

and each pair has exact number of unique values.

```{r}
names(instacart)

tibble(
  var = c("product_id", "product_name", 
          "aisle_id", "aisle" , 
          "department_id" , "department"),
  unique_var = c(n_distinct(pull(instacart,product_id)),
                 n_distinct(pull(instacart,product_name)),
                 n_distinct(pull(instacart,aisle_id)),
                 n_distinct(pull(instacart,aisle)),
                 n_distinct(pull(instacart,department_id)),
                 n_distinct(pull(instacart,department)))
) %>% knitr::kable()

```

Besides, we can also see number of unique `order_id` is equal to that of `user_id`, both are 131209. This means in this dataset, only one order is included for each unique user.

```{r}
instacart %>% 
  group_by(user_id) %>% 
  summarize(distinct_order = n_distinct(order_id)) %>% 
  arrange(distinct_order)
```

For each order, we can summarize some important features and plots as followed.

```{r,collapse = TRUE}
inst_order = instacart %>% 
  group_by(order_id) %>% 
  mutate(
    n_product = n_distinct(product_id)
    ) %>%
  select(order_id,order_number,order_dow,order_hour_of_day,n_product) %>% 
  distinct() %>% 
  relocate(order_id,n_product) 

knitr::kable(head(inst_order,10))

min(pull(inst_order,n_product))
max(pull(inst_order,n_product))
mean(pull(inst_order,n_product))

```

We can find that the minimum number of product bought is 1, maximum is 80, and average number is about 11. The number of products in seperate orders can be visualized as the following bar plot. 

```{r}
inst_order %>% 
  ggplot(aes(x=n_product))+ geom_histogram(fill = "4",alpha = .7)+
  labs(
    title = "Count of number of products purchased in seperate orders",
    x = "Number of products in seperate orders",
    y = "Count of orders"
  )
  
```

#### Questions

**1. How many aisles are there, and which aisles are the most items ordered from?**

```{r}
n_distinct(pull(instacart,aisle))

instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  arrange(by = desc(n_items))
```

From the above results, there are 134 aisles, and the top 2 aisles that have most items ordered from are `fresh vegetables` and `fresh fruits`.

**2. Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.**

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_items = n()) %>% 
  filter(n_items > 10000) %>% 
  mutate(aisle = fct_reorder(aisle,n_items),
         ) %>% 
  ggplot(aes(x = n_items, y = aisle))+
  geom_point(color = 4,size = 2, alpha = .6)+
  labs(
    title = "Number of items ordered in each aisle",
    subtitle = "(for number of items more than 10000)",
    x = "Number of items",
    y = "Aisle"
  )
```


**3. Make a table showing the three most popular items in each of the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits". Include the number of times each item is ordered in your table.**


```{r}
instacart %>% 
  select(aisle,product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(product_name) %>% 
  mutate(n_product = n())%>% 
  distinct() %>% 
  group_by(aisle) %>% 
  mutate(rank_within_aisle = min_rank(desc(n_product))) %>% 
  filter(rank_within_aisle == 1|rank_within_aisle ==2 | rank_within_aisle ==3) %>% 
  arrange(aisle,rank_within_aisle) %>% 
  knitr::kable()

```

The above table shows the three most popular items in each of the aisles "baking ingredients", "dog food care", and "packaged vegetables fruits". The number of times each item is ordered are presented in column `n_product`


**4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).**

```{r}
hour_week = instacart %>% 
  select(product_name, order_dow,order_hour_of_day) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name,order_dow) %>% 
  mutate(mean_hour = mean(order_hour_of_day)) %>% 
  select(-order_hour_of_day) %>% 
  distinct() %>% 
  pivot_wider(names_from = order_dow,values_from = mean_hour,names_sort = TRUE)
colnames(hour_week) = c("Products/Days","Sun","Mon","Tue","Wed","Thu","Fri","Sat")
  
knitr::kable(hour_week)

```

The above 2*7 table shows the mean hour of the day at which `Pink Lady Apples` and `Coffee Ice Cream` are ordered from Sun to Sat.


### Problem 2


#### Data Loading and Cleaning

```{r,collapse=TRUE}
data("brfss_smart2010")

brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health",
         response %in% c('Excellent','Very good','Good','Fair','Poor'))%>%
  mutate(
    response = factor(response,order = TRUE, 
        levels = c('Poor','Fair','Good','Very good','Excellent'))
    ) 
sum(is.na(brfss))

brfss = brfss %>% 
  select(-data_value_footnote_symbol,-data_value_footnote,-location_id)


sum(is.na(brfss))
```

After cleaning the columns names and filtering "Overall Health" and selected response levels, there are 31847 `NA` values, mostly in columns `data_value_footnote_symbol`,`data_value_footnote` and `location_id`. Since these columns are null or only have one kind of input, we drop the 3 columns for simplicity. After that, there only has 28 missing values, all line in `data_value`, which are kept temporarily until some analysis on this feature in later analysis.

First 10 lines of the processes dataset is shown below:

```{r}
knitr::kable(head(brfss,10))
```


#### Questions

**1. In 2002, which states were observed at 7 or more locations? What about in 2010?**

```{r}
states_2002 = brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(year = 2002,
            n_location = n_distinct(locationdesc)) %>% 
  filter(n_location >= 7)
knitr::kable(states_2002)

states_2010 = brfss %>% 
  filter(year == 2010) %>% 
  group_by(locationabbr) %>% 
  summarize(year = 2010,
            n_location = n_distinct(locationdesc)) %>% 
  filter(n_location >= 7)
knitr::kable(states_2010)
```

In 2002, states including ``r pull(states_2002,locationabbr)`` were observed at 7 or more locations;
In 2010, states including ``r pull(states_2010,locationabbr)`` were observed at 7 or more locations.



**2. Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a "spaghetti" plot of this average value over time within a state (that is, make a plot showing a line for each state across years â€“ the geom_line geometry and group aesthetic will help).**

We construct the required dataset as `excl_df`. `NA` values are omitted when calculating mean value of `data_value`. A preview of 10 lines was shown below.

```{r}

excl_df = brfss %>% 
  filter(response == "Excellent") %>% 
  select(year, locationabbr,data_value) %>% 
  group_by(locationabbr,year) %>% 
  mutate(mean_data_value = mean(data_value,na.rm = TRUE)) %>% 
  select(-data_value) %>% 
  distinct()

knitr::kable(head(excl_df,10))
```


A "spaghetti" plot the average value over year within a state is shown below.


```{r}
excl_df %>% 
  ggplot(aes(x = year, y = mean_data_value, group = locationabbr, color = locationabbr)) +
  geom_line()+
  labs(
    title = "Average data_value within a state",
    subtitle = "From Year 2002 - 2010",
    y = "average data_value within state"
  )+
  scale_color_viridis(name = "States",discrete = TRUE)

```



**3. Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses ("Poor" to "Excellent") among locations in NY State.**

The following violin plot shows the differences in distribution of `data_value` for `response`s 'Poor','Fair','Good','Very good','Excellent' between 2006 and 2010 among locations in NY State.

```{r}
brfss %>% 
  filter(year == 2006 | year == 2010,
         locationabbr == "NY") %>% 
  select(year,response,data_value) %>% 
  ggplot(aes( x = data_value,y = response, fill = response))+
  geom_density_ridges(alpha = .6,scale = 0.8) +
  #geom_dist(aes(fill = response))+
  facet_grid(.~year)+
  labs(
    title = "Distribution of data_value for responses from \"Poor\" to \"Excellent\"",
    subtitle = "A comparison between Year 2006 and 2010 among locations in NY State",
    x = "Responses",
    y = "Data_value of locations within NY state"
  )
```


### Problem 3

#### Loading and tidying data

```{r}
accel_df = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    weekend = ifelse(day %in% c("Saturday","Sunday"),1,0),
    ) %>% 
  pivot_longer(4:1443,names_to = "minute", names_prefix = "activity_",values_to = "activity_count") %>% 
  mutate(
    weekend = as.factor(weekend),
    minute = as.numeric(minute)
    ) 
```


#### 1. 
After data wrangling, the resulting dataframe has `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. The variables include:

* `week`: numeric variable from the original data, contains 5 integers from 1 to 5.
* `day_id`: numeric variable from the original data, contains 35 integers from 1 to 35.
* `day`: `day` variable from the original data, is a character variable containing 7 unique values `r unique(pull(accel_df,day))`
* `weekend`: a newly created factor variable containing factors `1` and `0`, where `1` stands for weekend, and `0` stands for weekdays.
* `minute`: a newly created numeric variable by `pivot_longer` of the original variable `activity.*`. It stands for the minute when the activity occurs.
* `activity_count`: another newly created numeric variable by `pivot_longer` of the original variable `activity.*`. It contains the original activity counts for each minute.

An overview of first 10 lines of the resulting dataset is shown below:

```{r}
knitr::kable(head(accel_df,10))
```


Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

#### 2. 
The above scatter plot shows the total activity counts calculated from the `accel_df` dataset over days.

```{r}
accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity_count = sum(activity_count)) %>% 
  ggplot(aes(x = day_id, y = total_activity_count))+
  geom_point(color = 4,size = 3,alpha = .6)+
  labs(
    title = "Total activity counts over days",
    x = "Day",
    y = "Total activity counts"
  )+scale_y_continuous(
    breaks = c(0,200000,400000,600000),
    labels = c(0,'200,000','400,000','600,000')
  )
```


From the plot, total activity counts seem to jump up and down across days and does not have apparent daily trends. 

#### 3.

Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

To generate the 24-hour activity time courses for each day, convert minute to hours by taking the integer part of `minute/60`. For `minute = 14400`, categorize it into `hour = 0`. The following plot shows the 24-hour activity time courses for each day. Each line represents activity variation for one day in different hours, and different colors represent for day of the week. 


```{r}
accel_df %>% 
  mutate(hour_0_24 = floor(minute/60),
         hour = ifelse(hour_0_24 == 24, 0, hour_0_24),
         day = factor(day,levels = c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))) %>% 
  group_by(hour,day_id,day) %>%
  summarize(activity_count_hour = sum(activity_count)) %>%
  ggplot(aes(x = hour, y = activity_count_hour,group = day_id,color = day))+
  geom_point(alpha = .8)+
  geom_line(alpha = .5,size = .8)+labs(
    title = "24-hour activity counts for 35 measurement days",
    y = "activities counts in one hour break"
  )
  
  
  
```

From the plot, we can see that in most days, activity accounts are less during the night from 22pm to 6am, and are more in the rest hour intervals, which quite make sense considering the real life routine of a person. But there's no clear pattern regarding activity counts in different days of week.












